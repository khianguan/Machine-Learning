{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder , LabelEncoder\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV , RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier , RandomForestClassifier , AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix # creates a confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix # draws a confusion matrix\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario:** You work at a multinational bank that is aiming to increase it's market share in \n",
    "Europe. Recently, it has been noticed that the number of customers using the banking \n",
    "services has declined, and the bank is worried that existing customers have stopped \n",
    "using them as their main bank. <br> \n",
    "\n",
    "As a data scientist, you are tasked with finding out the \n",
    "reasons behind customer churn (when a customer stops using them as the main bank) and to predict customer churn. <br> \n",
    "\n",
    "The marketing team, \n",
    "in particular, is interested in your findings and want to better understand existing \n",
    "customer behavior and possibly predict customer churn. Your results will help the \n",
    "marketing team to use their budget wisely to target potential churners. To achieve \n",
    "this objective, in this exercise, you will import the banking data (Churn_Modelling.csv) \n",
    "provided by the bank and do some machine learning to solve their problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data dictionary\n",
    "\n",
    "- CustomerID: Unique ID of each customer\n",
    "- CredRate: Credit Score of the customer \n",
    "- Geography: Country customer is from \n",
    "- Gender\n",
    "- Age\n",
    "- Tenure: How long customer has been with bank \n",
    "- Balance: Amount of money customer has/had with the bank\n",
    "- Prod Number: Number of products customer has with bank \n",
    "- HasCrCard: Does customer have credit card\n",
    "- ActMem: Is customer active member \n",
    "- Estimated salary: Annual estimated salary of customer \n",
    "- Exited: Whether customer has churned (1 is yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: To better understand existing customer behavior and prediict potential churners to better utilize marketing budget.\n",
    "\n",
    "Input Features: \"CredRate\", \"Gender\", \"Age\", \"Tenure\", \"Prod Number\", \"HasCrCard\", \"ActMem\" and \"Estimated salary\"\n",
    "\n",
    "Drop Features: \"CustomerID\", \"Geography\" and \"Exited\" (prediction target)\n",
    "\n",
    "Prediction Target: \"Exited\" variable - To reduce \"1\" (customer churn rate) and increase \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Understanding and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data\n",
    "data = pd.read_csv(\"Churn_Modelling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strip spaces in Variable headers as it will cause error in coding - Prod Number and Estimated Salary\n",
    "data.columns = data.columns.str.replace(' ', '') \n",
    "#Check DData\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking for null values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the 14 null data - 13 (Active = 0) vs 1 (Churned = 1)\n",
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the NaN features - Gender, Age and Estimated Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,13))\n",
    "sns.heatmap(data.corr() , annot= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No particularly closely correlated variables for Age, Gender and Estimated Salary to replace the data.\n",
    "\n",
    "Let's look at the data statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No particular skewness observed for Age data.\n",
    "\n",
    "Mean = 38,\n",
    "Median = 37\n",
    "\n",
    "No particular skewness observed for Estimated Salary data.\n",
    "\n",
    "Mean = 100,074,\n",
    "Median = 100,168\n",
    "\n",
    "Let's fill Age and Estimated Salary NaN values with their Mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'] = data['Age'].fillna(data['Age'].median())\n",
    "data['EstimatedSalary'] = data['EstimatedSalary'].fillna(data['EstimatedSalary'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = data.groupby(['Gender','Exited']).size().reset_index().pivot(columns='Exited', index='Gender', values=0)\n",
    "df.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No particular skewness observed for Gender data. Female have a higher tendency to churn over Male. \n",
    "\n",
    "Fill the NA gender with mode (highest frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Gender'] = data['Gender'].fillna(data['Gender'].mode()[0])\n",
    "# data['Gender'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values again\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning for EDA - Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age_bins'] = pd.cut(x=data['Age'], bins=[0, 29, 39, 49, 59, 99])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.groupby(['age_bins','Exited']).size().reset_index().pivot(columns='Exited', index='age_bins', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(7,5))\n",
    "df.plot(figsize=(7,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Churn Rate is high for customers in their 40s (%), followed by 50s (%), then 30s (%). These are our prime spenders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning for EDA - Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['salary_bins'] = pd.cut(x=data['EstimatedSalary'], bins=[0,30000,60000,90000,120000,150000,200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.groupby(['salary_bins','Exited']).size().reset_index().pivot(columns='Exited', index='salary_bins', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(7,5))\n",
    "df.plot(figsize=(7,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Churn Rate is pretty consistent across different Salary ranges with the exception of slightly higher churn rate observed for >150k Salary (highest spending power).\n",
    "Action must be taken to retain these customers with higher spending power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.groupby(['ProdNumber','Exited']).size().reset_index().pivot(columns='Exited', index='ProdNumber', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(7,5))\n",
    "df.plot(figsize=(7,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet Spot seems to be at 2 products. Should X-Sell customer to 2 products then stop. Cm with higher than 3 products have a higher tendency to exit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning for EDA - Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bal_bins'] = pd.cut(x=data['Balance'], bins=[0,60000,90000,120000,150000,180000,260000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = data.groupby(['bal_bins','Exited']).size().reset_index().pivot(columns='Exited', index='bal_bins', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(7,5))\n",
    "df.plot(figsize=(7,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trend seems to follow normal distribution with customers having 90k-150k balance with the highest churn rate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age X Salary (Active) - Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data0=data[data.Exited==0]\n",
    "df = data0.groupby(['salary_bins','age_bins']).size().reset_index().pivot(columns='salary_bins', index='age_bins', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(10,7))\n",
    "df.plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age X Salary (Exited) - Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data1=data[data.Exited==1]\n",
    "df = data1.groupby(['salary_bins','age_bins']).size().reset_index().pivot(columns='salary_bins', index='age_bins', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(10,7))\n",
    "df.plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at those who has churned (chart above), there must be action taken to recover these prime age group (30s and 40s) and high income earners (90k and above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age X ProdNumber (Active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0=data[data.Exited==0]\n",
    "df = data0.groupby(['ProdNumber','age_bins']).size().reset_index().pivot(columns='ProdNumber', index='age_bins', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(10,7))\n",
    "df.plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age X ProdNumber (Exited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data[data.Exited==1]\n",
    "df = data1.groupby(['ProdNumber','age_bins']).size().reset_index().pivot(columns='ProdNumber', index='age_bins', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(10,7))\n",
    "df.plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.groupby(['ProdNumber','bal_bins']).size().reset_index().pivot(columns='ProdNumber', index='bal_bins', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(10,6))\n",
    "df.plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance X ProdNumber (Active)  - Existing Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0=data[data.Exited==0]\n",
    "df = data0.groupby(['ProdNumber','bal_bins']).size().reset_index().pivot(columns='ProdNumber', index='bal_bins', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(10,6))\n",
    "df.plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer who stays with the bank mainly holds 1-2 products. Shouldn't be too agressive to upsell customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance X ProdNumber (Exited)  - Existing Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data[data.Exited==1]\n",
    "df = data1.groupby(['ProdNumber','bal_bins']).size().reset_index().pivot(columns='bal_bins', index='ProdNumber', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(10,6))\n",
    "df.plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should focus on x-selling customer with 90k to 150k balance from 1 product to 2 products to reduce churn rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = sns.barplot(x=\"ProdNumber\", y=\"Balance\", hue=\"Exited\", data=data, estimator=sum)\n",
    "ax1.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = sns.barplot(x=\"ProdNumber\", y=\"Balance\", data=data1, estimator=sum)\n",
    "ax1.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age X Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data0.groupby(['bal_bins','age_bins']).size().reset_index().pivot(columns='bal_bins', index='age_bins', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(10,7))\n",
    "df.plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "ax = sns.barplot(x=\"age_bins\", y=\"Balance\", hue=\"Exited\", data=data, estimator=sum)\n",
    "ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax1 = sns.barplot(x=\"age_bins\", y=\"Balance\", data=data1, estimator=sum)\n",
    "ax1.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Close to $170MM of funds exited!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data[data.Exited==1]\n",
    "df = data1.groupby(['bal_bins','age_bins']).size().reset_index().pivot(columns='bal_bins', index='age_bins', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(10,7))\n",
    "df.plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cr_bins'] = pd.cut(x=data['CredRate'], bins=[300,579,669,739,799,850])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = data.groupby(['cr_bins','Exited']).size().reset_index().pivot(columns='Exited', index='cr_bins', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(10,5))\n",
    "df.plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total'] = df[0]+df[1]\n",
    "df['perc0'] = df[0].div(df[0].sum(),0)*100\n",
    "df['perc1'] = df[1].div(df[1].sum(),0)*100\n",
    "df['percT'] = df['total'].div(df.total.sum(),0)*100\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df[0]+df[1]\n",
    "df_rel = df[df.columns[:2]].div(df_total, 0)*100\n",
    "df_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One of the most well-known types of credit score are FICO® Scores, created by the Fair Isaac Corporation. FICO® Scores are used by many lenders, and often range from 300 to 850.\n",
    "\n",
    "Credit Score (Rating): % - Impact\n",
    "\n",
    "300-579\t(Very Poor): 24% - Credit applicants may be required to pay a fee or deposit, and applicants with this rating may not be approved for credit at all.\n",
    "\n",
    "580-669\t(Fair): 33% - Applicants with scores in this range are considered to be subprime borrowers.\n",
    "\n",
    "670-739\t(Good): 24% - Studies shown that only 8% of applicants in this score range are likely to become seriously delinquent in the future.\n",
    "\n",
    "740-799\t(Very Good): 12% - Applicants with scores here are likely to receive better than average rates from lenders.\n",
    "\n",
    "800-850\t(Exceptional): 7% - Applicants with scores in this range are at the top of the list for the best rates from lenders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights/Analysis\n",
    "From the above score with a range between 300-850, more than half of the banks customers (57%) are considered to be subprime borrowers, we should consider targetting customer whose cred score is Very Good for sharper targeting and lower costs.\n",
    "\n",
    "Might consider the Good Credit Rating for a larger base targetting. Depending on what the bank intends to go for.\n",
    "\n",
    "Spend Revenue? or Loan Revenue?\n",
    "Or Retail Spend/Investment Revenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = data.groupby(['HasCrCard','Exited']).size().reset_index().pivot(columns='Exited', index='HasCrCard', values=0)\n",
    "df.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HasCrCard ~ mehh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = data.groupby(['Tenure','Exited']).size().reset_index().pivot(columns='Exited', index='Tenure', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenure ~ Consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.groupby(['Geography','Exited']).size().reset_index().pivot(columns='Exited', index='Geography', values=0)\n",
    "df.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.groupby(['ActMem','Exited']).size().reset_index().pivot(columns='Exited', index='ActMem', values=0)\n",
    "df.plot(kind='bar', stacked=True, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher tendency to churn when you are inactive - Logical. Coincides with XGBoost Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you have more time/resources/data - look at what customer product he is currently having, then apply a model to offer the next best product for customer  to push customer from 1 product to 2 product to increase stickiness.\n",
    "We should also consider acquisition instead of looking to squeeze more from our existing customers as the incremental revenue will always be higher for newly acquired customers.\n",
    "We could also offer differential pricing for customers with better credit ratings to incentivise them to take up a loan/spend more to increase the banks net revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So do you want to move to higher wealth cm or just stay within mass market?\n",
    "What are other factors that cause the cm to choose the bank? pricing/int rate? Service? ATM locations/accessibility?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preform necessary Preprocessing steps , so that it is prepared for model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc = pd.get_dummies(data['Gender'])\n",
    "\n",
    "data = pd.concat([data, df_enc], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc = pd.get_dummies(data['Geography'])\n",
    "\n",
    "data = pd.concat([data, df_enc], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop(['CustomerId','Exited','Geography','Male','Gender','age_bins','bal_bins','salary_bins','cr_bins'] , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data.Exited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (14 , 6))\n",
    "sns.boxplot(data= features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate features and target: target is Exited. Do a train test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(features, target, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalar_method = StandardScaler()\n",
    "\n",
    "scaled_X = scalar_method.fit_transform(features)\n",
    "\n",
    "scaled_data = pd.DataFrame(scaled_X , columns= features.columns)\n",
    "\n",
    "scaled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (14 , 7))\n",
    "sns.boxplot(data= scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (14 , 7))\n",
    "sns.boxplot(data= X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (14 , 7))\n",
    "sns.boxplot(data= X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split is approximately the same. Which is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rfc = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rfc = model_rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, result_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Exited.value_counts(1)\n",
    "#Target data is fairly skewed with ~80% of active and 20% churn rate. No point looking at accuracy.\n",
    "#We need to focus on predicting false positive for area under curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(1)\n",
    "#Checking our y Test to ensure there's distribution of 1 (churned customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, result_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, result_rfc)\n",
    "ax=sns.heatmap(cm, annot= True, cmap=\"Blues\",cbar=False, fmt='g')\n",
    "plt.xlabel(\"predicted\", va = 'top')\n",
    "plt.ylabel(\"true\")\n",
    "# plt.title('confusion matrix') \n",
    "ax.xaxis.set_label_position('top')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "area_under_curve = roc_auc_score(y_test, result_rfc)\n",
    "\n",
    "print(area_under_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve plot\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, model_rfc.predict_proba(X_test)[:,1])  # second argument = positive class predictions\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.plot(fpr, tpr, label='Random Forest (area = %0.2f)' %area_under_curve)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFC is already giving quite a good model score! But we should consider logistic regression and GaussianNB which are also catered for binary classification - such as predicting if customer will churn/not churn before spending the marketing budget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x= 'Age', y= 'Exited', data= data, logistic= True).set_title(\"Age Log Odds Linear Plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age seems like a good fit for logistic regression modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "Model_LR = LogisticRegression()\n",
    "Model_LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "LR_pred = Model_LR.predict(X_test)\n",
    "LR_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, LR_pred)\n",
    "ax=sns.heatmap(cm, annot= True, cmap=\"Blues\", cbar=False, fmt='g')\n",
    "plt.xlabel(\"predicted\", va = 'top')\n",
    "plt.ylabel(\"true\")\n",
    "# plt.title('confusion matrix') \n",
    "ax.xaxis.set_label_position('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, LR_pred))\n",
    "\n",
    "# AUC score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "area_under_curve = roc_auc_score(y_test, Model_LR.predict(X_test))\n",
    "print(area_under_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, Model_LR.predict_proba(X_test)[:,1])  # second argument = positive class predictions\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.plot(fpr, tpr, label='Logistic Classifier (area = %0.2f)' % area_under_curve)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "ModelGNB = GaussianNB()\n",
    "ModelGNB.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "GNB_pred = ModelGNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, GNB_pred)\n",
    "ax=sns.heatmap(cm, annot= True, cmap=\"Blues\", cbar=False, fmt='g')\n",
    "plt.xlabel(\"predicted\", va = 'top')\n",
    "plt.ylabel(\"true\")\n",
    "# plt.title('confusion matrix') \n",
    "ax.xaxis.set_label_position('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, GNB_pred))\n",
    "# AUC score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "area_under_curve = roc_auc_score(y_test, ModelGNB.predict(X_test))\n",
    "print(area_under_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, ModelGNB.predict_proba(X_test)[:,1])  # second argument = positive class predictions\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.plot(fpr, tpr, label='GaussianNB (area = %0.2f)' % area_under_curve)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model_xgb = xgb.XGBClassifier(objective = 'binary:logistic', random_state = 42)\n",
    "\n",
    "model_xgb = model_xgb.fit(X_train, y_train)\n",
    "result_xgb = model_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, result_xgb)\n",
    "ax=sns.heatmap(cm, annot= True, cmap=\"Blues\", cbar=False, fmt='g')\n",
    "plt.xlabel(\"predicted\", va = 'top')\n",
    "plt.ylabel(\"true\")\n",
    "# plt.title('confusion matrix') \n",
    "ax.xaxis.set_label_position('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, result_xgb))\n",
    "area_under_curve = roc_auc_score(y_test, model_xgb.predict(X_test))\n",
    "print(area_under_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, model_xgb.predict_proba(X_test)[:,1])  # second argument = positive class predictions\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.plot(fpr, tpr, label='XGBoost (area = %0.2f)' % area_under_curve)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GB = GradientBoostingClassifier(random_state = 42)\n",
    "model_GB = model_GB.fit(X_train, y_train)\n",
    "result_GB= model_GB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, result_GB))\n",
    "area_under_curve = roc_auc_score(y_test, model_GB.predict(X_test))\n",
    "print(area_under_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, result_GB)\n",
    "ax=sns.heatmap(cm, annot= True, cmap=\"Blues\", cbar=False, fmt='g')\n",
    "plt.xlabel(\"predicted\", va = 'top')\n",
    "plt.ylabel(\"true\")\n",
    "# plt.title('confusion matrix') \n",
    "ax.xaxis.set_label_position('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, model_GB.predict_proba(X_test)[:,1])  # second argument = positive class predictions\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.plot(fpr, tpr, label='XGBoost (area = %0.2f)' % area_under_curve)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL Training/Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict_RFC = {\"n_estimators\" : [64 , 96 , 128] , \n",
    "               \"max_depth\" : [6 , 9, 12],\n",
    "               \"max_features\" : [5, 7, 9], \n",
    "               \"criterion\" : ['entropy','gini'], \n",
    "               \"bootstrap\" : [True , False ] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict_RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_modelrfc = GridSearchCV(estimator=model_rfc, param_grid= param_dict_RFC , cv=10, n_jobs=-1 , verbose=1) \n",
    "# n_jobs means use all processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_modelrfc.fit(X_train , y_train)\n",
    "\n",
    "grid_modelrfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rfc_tuned = grid_modelrfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, result_rfc_tuned))\n",
    "area_under_curve = roc_auc_score(y_test, grid_modelrfc.predict(X_test))\n",
    "print(area_under_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, result_rfc_tuned)\n",
    "ax=sns.heatmap(cm, annot= True, cmap=\"Blues\", cbar=False, fmt='g')\n",
    "plt.xlabel(\"predicted\", va = 'top')\n",
    "plt.ylabel(\"true\")\n",
    "# plt.title('confusion matrix') \n",
    "ax.xaxis.set_label_position('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, grid_modelrfc.predict_proba(X_test)[:,1])  # second argument = positive class predictions\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.plot(fpr, tpr, label='RFC Tuned (area = %0.2f)' % area_under_curve)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Tuning for RFC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adaboost = AdaBoostClassifier(base_estimator= model_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adaboost = model_adaboost.fit(X_train, y_train)\n",
    "result_ada= model_adaboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, result_ada))\n",
    "area_under_curve = roc_auc_score(y_test, model_adaboost.predict(X_test))\n",
    "print(area_under_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict_ada =  {'n_estimators' : [16,32,64] \n",
    "                 , 'learning_rate' : [0.1, 0.5, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model_ada = GridSearchCV(param_grid= param_dict_ada , \n",
    "                              estimator= model_adaboost, n_jobs=-1,\n",
    "                              cv= 10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model_ada.fit(X_train, y_train)  # takes some time to train..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model_ada.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ada_Tuned = grid_model_ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, result_ada_Tuned))\n",
    "area_under_curve = roc_auc_score(y_test, grid_model_ada.predict(X_test))\n",
    "print(area_under_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, result_ada_Tuned)\n",
    "ax=sns.heatmap(cm, annot= True, cmap=\"Blues\", cbar=False, fmt='g')\n",
    "plt.xlabel(\"predicted\", va = 'top')\n",
    "plt.ylabel(\"true\")\n",
    "# plt.title('confusion matrix') \n",
    "ax.xaxis.set_label_position('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, grid_model_ada.predict_proba(X_test)[:,1])  # second argument = positive class predictions\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.plot(fpr, tpr, label='RFC Adaboost (area = %0.2f)' % area_under_curve)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_rfc.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "feat_importances = pd.Series(model_rfc.feature_importances_, index=features.columns)\n",
    "feat_importances.nlargest(15).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "feat_importances = pd.Series(model_xgb.feature_importances_, index=features.columns)\n",
    "feat_importances.nlargest(15).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [6,9,12],\n",
    "    'max_delta_step' : [0, 1, 2], \n",
    "    'learning_rate': [0.00001, 0.00005, 0.0001],\n",
    "    'reg_lambda': [250, 300, 350],\n",
    "    'gamma': [25, 50, 75],\n",
    "    'scale_pos_weight': [4,5]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params = GridSearchCV(\n",
    "                    estimator = model_xgb,\n",
    "#                     objective = 'binary:logistic',\n",
    "                    param_grid = param_grid,\n",
    "                    scoring = 'f1', ## f1 see https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "                    verbose = 1, \n",
    "                    cv = 10\n",
    ")\n",
    "\n",
    "optimal_params.fit(X_train, \n",
    "                   y_train, \n",
    "                   early_stopping_rounds=10,   \n",
    "                   eval_set=[(X_test, y_test)],   # evaluate auc upon the test set\n",
    "                   verbose=False)\n",
    "\n",
    "print(optimal_params.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_xgbt = optimal_params.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, result_xgbt))\n",
    "area_under_curve = roc_auc_score(y_test, optimal_params.predict(X_test))\n",
    "print(area_under_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, result_xgbt)\n",
    "ax=sns.heatmap(cm, annot= True, cmap=\"Blues\", cbar=False, fmt='g')\n",
    "plt.xlabel(\"predicted\", va = 'top')\n",
    "plt.ylabel(\"true\")\n",
    "# plt.title('confusion matrix') \n",
    "ax.xaxis.set_label_position('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, optimal_params.predict_proba(X_test)[:,1])  # second argument = positive class predictions\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.plot(fpr, tpr, label='XGB Adaboost (area = %0.2f)' % area_under_curve)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Tuning for XGBoostt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adaboostxgb = AdaBoostClassifier(base_estimator= model_xgb, \n",
    "                               n_estimators= 16 , learning_rate= 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adaboostxgb = model_adaboostxgb.fit(X_train, y_train)\n",
    "result_adaxgb= model_adaboostxgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, result_adaxgb))\n",
    "area_under_curve = roc_auc_score(y_test, model_adaboostxgb.predict(X_test))\n",
    "print(area_under_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict_adaxgb =  {'n_estimators' : [4,10,16] \n",
    "                 , 'learning_rate' : [0.01, 0.05, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model_adaxgb = GridSearchCV(param_grid= param_dict_adaxgb , \n",
    "                              estimator= model_adaboost, n_jobs=-1,\n",
    "                              cv= 10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model_adaxgb.fit(X_train, y_train)  # takes some time to train..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model_adaxgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_adaxgb_Tuned = grid_model_adaxgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, result_adaxgb_Tuned))\n",
    "area_under_curve = roc_auc_score(y_test, grid_model_adaxgb.predict(X_test))\n",
    "print(area_under_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, result_adaxgb_Tuned)\n",
    "ax=sns.heatmap(cm, annot= True, cmap=\"Blues\", cbar=False, fmt='g')\n",
    "plt.xlabel(\"predicted\", va = 'top')\n",
    "plt.ylabel(\"true\")\n",
    "# plt.title('confusion matrix') \n",
    "ax.xaxis.set_label_position('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, grid_model_adaxgb.predict_proba(X_test)[:,1])  # second argument = positive class predictions\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.plot(fpr, tpr, label='RFC Adaboost (area = %0.2f)' % area_under_curve)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately depend on your marketing $$ budget\n",
    "\n",
    "Using RF (precision): By spending $251(189+62), you can salvage 189/(189+62) = 75% cm salvage. (precision measure - reduce false positives)\n",
    "\n",
    "Using RF (Recall): By spending $393(189+204), you can salvage 189/(189+204) = 48% cm salvage. (recall measure - reduce false negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control Overfitting\n",
    "When you observe high training accuracy, but low test accuracy, it is likely that you encountered overfitting problem.\n",
    "\n",
    "There are in general two ways that you can control overfitting in XGBoost:\n",
    "\n",
    "The first way is to directly control model complexity.\n",
    "\n",
    "This includes max_depth, min_child_weight and gamma.\n",
    "\n",
    "The second way is to add randomness to make training robust to noise.\n",
    "\n",
    "This includes subsample and colsample_bytree.\n",
    "\n",
    "You can also reduce stepsize eta. Remember to increase num_round when you do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
